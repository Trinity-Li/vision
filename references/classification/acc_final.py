import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import sys
import glob
import re
import csv

# --- å¯¼å…¥åŒçº§ç›®å½•ä¸‹çš„å®˜æ–¹æ¨¡å— ---
try:
    import utils
    from train import train_one_epoch, evaluate
except ImportError:
    print("âŒ é”™è¯¯ï¼šè¯·å°†æ­¤è„šæœ¬æ”¾åœ¨ 'references/classification/' ç›®å½•ä¸‹è¿è¡Œï¼")
    sys.exit(1)

# ==========================================
# 1. é…ç½®åŒºåŸŸ
# ==========================================
TRAIN_FILES_PATTERN = 'eval_set_B*.pt'  # è®­ç»ƒé›†ç´¢å¼•æ–‡ä»¶æ¨¡å¼
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ä½ çš„ Checkpoint è·¯å¾„ (å¦‚æœéœ€è¦ä»ä¹‹å‰çš„æƒé‡å¼€å§‹ï¼Œè¯·å¡«è¿™é‡Œ)
# å¦‚æœç•™ç©ºï¼Œå°†é»˜è®¤ä½¿ç”¨ ImageNet é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–
PRETRAINED_CHECKPOINT = '/root/autodl-tmp/eval/references/classification/checkpoint.pth'


class Args:
    def __init__(self):
        self.batch_size = 128
        self.epochs = 15  # é’ˆå¯¹ CIFAR-10 ç»“æ„è°ƒæ•´åï¼Œè®­ç»ƒå¯èƒ½éœ€è¦ç¨å¤šå‡ è½®
        self.lr = 0.000001  # SGD åˆå§‹å­¦ä¹ ç‡
        self.momentum = 0.9
        self.weight_decay = 5e-4  # CIFAR-10 å¸¸ç”¨ weight_decay
        self.print_freq = 50
        self.workers = 8
        self.output_dir = '.'

        # å®˜æ–¹ä»£ç å ä½å‚æ•°
        self.clip_grad_norm = None
        self.model_ema = False
        self.lr_warmup_epochs = 0
        self.lr_warmup_method = 'linear'
        self.lr_warmup_decay = 0.01
        self.label_smoothing = 0.0
        self.mixup_alpha = 0.0
        self.cutmix_alpha = 0.0
        self.distributed = False
        self.rank = 0


# ==========================================
# 2. CIFAR-10 ä¸“ç”¨ Transforms
# ==========================================
def get_cifar_transforms(is_train=True):
    # CIFAR-10 æ ‡å‡†å‡å€¼å’Œæ–¹å·®
    mean = (0.4914, 0.4822, 0.4465)
    std = (0.2023, 0.1994, 0.2010)

    if is_train:
        return transforms.Compose([
            transforms.RandomCrop(32, padding=4),  # ä¿æŒ 32x32
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
    else:
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])


# ==========================================
# 3. è‡ªå®šä¹‰æ•°æ®é›† (åŒå‰)
# ==========================================
class FilelistDataset(Dataset):
    def __init__(self, pt_path, transform=None):
        self.samples = []
        self.transform = transform

        # ========================================================
        # 1. å®šä¹‰æ–°æ•°æ®çš„æ ¹ç›®å½• (å³ä½ åˆšæ‰è¿è¡Œ restore è„šæœ¬ç”Ÿæˆçš„ç›®å½•)
        # ========================================================
        SPLIT_DATA_ROOT = '/root/autodl-tmp/eval/references/classification/cifar10_split_data'

        # CIFAR-10 ç±»åˆ«å (ç”¨äºæ„å»ºè·¯å¾„)
        CIFAR_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

        # 2. è‡ªåŠ¨åˆ¤æ–­æ˜¯ Set A è¿˜æ˜¯ Set B
        # æ ¹æ® .pt æ–‡ä»¶åæ¥çŒœã€‚æ–‡ä»¶åé‡Œé€šå¸¸åŒ…å« 'set_B' æˆ– 'set_A'
        if 'set_B' in os.path.basename(pt_path):
            subset_folder = 'eval_set_B'
        else:
            subset_folder = 'eval_set_A'

        print(f"ğŸ“‚ è¯»å–ç´¢å¼•: {os.path.basename(pt_path)} -> æŒ‡å‘ç›®å½•: {subset_folder}")

        # 3. åŠ è½½ .pt æ–‡ä»¶
        try:
            data = torch.load(pt_path, map_location='cpu')
        except Exception as e:
            print(f"âŒ .pt æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return

        # 4. æå–æ•°æ®åˆ—è¡¨ (å…¼å®¹ä¸åŒæ ¼å¼)
        items = []
        if isinstance(data, dict):
            # ä½ çš„æ–‡ä»¶å¯èƒ½æ˜¯ {'ids': [...], 'labels': [...]} æˆ–è€…æ˜¯ {'features': ...}
            if 'ids' in data:
                ids = data['ids']
                lbls = data['labels']
            elif 'features' in data:  # å…¼å®¹ä½ ä»£ç é‡Œçš„è¿™ç§å†™æ³•
                ids = data['ids']
                lbls = data['labels']
            else:
                print("âŒ å­—å…¸æ ¼å¼æ— æ³•è¯†åˆ«ï¼Œç¼ºå°‘ 'ids'")
                return

            for i in range(len(ids)):
                items.append({'id': ids[i], 'label': int(lbls[i])})

        elif isinstance(data, list):
            items = data

        # 5. æ ¸å¿ƒå¾ªç¯ï¼šæ„å»ºæ–°è·¯å¾„
        for item in items:
            raw_id = item.get('id')
            label = int(item.get('label'))
            if not raw_id: continue

            # --- å…³é”®ä¿®æ”¹å¼€å§‹ ---
            # åŸå§‹å¯èƒ½æ˜¯: "/.../23182_r0.2_copy90.png"
            base_name = os.path.basename(raw_id)

            # ä½¿ç”¨æ­£åˆ™æå–å¼€å¤´çš„æ•°å­— ID
            # é€»è¾‘ï¼šæ‰¾åˆ°å­—ç¬¦ä¸²å¼€å¤´çš„ç¬¬ä¸€ä¸²æ•°å­—
            match = re.match(r'^(\d+)', base_name)

            if match:
                clean_id = match.group(1)  # æ‹¿åˆ° "23182"
                filename = f"{clean_id}.png"  # å¼ºåˆ¶å˜æˆ "23182.png"
            else:
                # å¦‚æœæ²¡åŒ¹é…åˆ°æ•°å­—ï¼Œå›é€€åˆ°åŸå§‹æ–‡ä»¶åï¼ˆé˜²æ­¢æŠ¥é”™ï¼Œè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰
                filename = base_name

            # --- å…³é”®ä¿®æ”¹ç»“æŸ ---

            # --- C. è·å–ç±»åˆ«å ---
            class_name = CIFAR_CLASSES[label]  # æ‹¿åˆ° "truck"

            # --- D. æ‹¼è£…æˆæ­£ç¡®çš„æ–°è·¯å¾„ ---
            # æ–°è·¯å¾„ = æ ¹ç›®å½• / eval_set_B / truck / 36717.png
            real_path = os.path.join(SPLIT_DATA_ROOT, subset_folder, class_name, filename)

            self.samples.append((real_path, label))

        print(f"   âœ… æˆåŠŸç´¢å¼• {len(self.samples)} å¼ å›¾ç‰‡")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        # 1. è·å–è·¯å¾„å’Œæ ‡ç­¾
        path, label = self.samples[idx]

        # 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (è¿™ä¸€æ­¥éå¸¸å…³é”®)
        if not os.path.exists(path):
            # è¯¦ç»†æŠ¥é”™ï¼Œå‘Šè¯‰ä½ åˆ°åº•ç¼ºå“ªä¸ªæ–‡ä»¶
            raise FileNotFoundError(
                f"ğŸ”¥ å›¾ç‰‡ç¼ºå¤±ï¼\n"
                f"  è¯•å›¾è¯»å–: {path}\n"
                f"  åŸå§‹ç´¢å¼•: {self.samples[idx]}\n"
                f"  è¯·æ£€æŸ¥ /root/autodl-tmp/eval/references/classification/cifar10_split_data ä¸‹æ˜¯å¦æœ‰æ–‡ä»¶ï¼Ÿ"
            )

        # 3. å®‰å…¨æ‰“å¼€å›¾ç‰‡
        try:
            img = Image.open(path).convert('RGB')
        except Exception as e:
            raise RuntimeError(f"ğŸ”¥ å›¾ç‰‡æŸåæ— æ³•æ‰“å¼€: {path}\n  é”™è¯¯ä¿¡æ¯: {e}")

        if self.transform:
            img = self.transform(img)

        return img, label

# ==========================================
# 4. æ¨¡å‹ä¿®æ”¹å‡½æ•°
# ==========================================
# ==========================================
# 4. æ¨¡å‹ä¿®æ”¹å‡½æ•° (ä¿®æ­£ç‰ˆ)
# ==========================================
def get_cifar_resnet18(pretrained_path=''):
    # 1. æ— è®ºå¦‚ä½•ï¼Œå…ˆåˆå§‹åŒ–ä¸€ä¸ªæ ‡å‡† ResNet
    # å¦‚æœæœ‰è‡ªå®šä¹‰æƒé‡ï¼Œè¿™é‡Œå¯ä»¥ç”¨ weights=None çº¯å‡€åˆå§‹åŒ–
    # å¦‚æœæ²¡æœ‰ï¼Œç”¨ 'DEFAULT' åŠ è½½ ImageNet æƒé‡ä½œä¸ºè¿ç§»å­¦ä¹ çš„åŸºç¡€
    init_weights = None if pretrained_path else 'DEFAULT'
    model = torchvision.models.resnet18(weights=init_weights)

    # 2. ã€å…³é”®ã€‘å…ˆè¿›è¡Œæ¶æ„ä¿®æ”¹ï¼ŒæŠŠå‘ä½è°ƒæ•´å¥½
    # ä¿®æ”¹ç¬¬ä¸€å±‚å·ç§¯ï¼šé€‚åº” 32x32 è¾“å…¥ (7x7 -> 3x3)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)

    # ç§»é™¤ç¬¬ä¸€å±‚æ± åŒ–
    model.maxpool = nn.Identity()

    # ä¿®æ”¹å…¨è¿æ¥å±‚ï¼šé€‚åº” 10 ç±»
    model.fc = nn.Linear(model.fc.in_features, 10)

    # 3. ã€å…³é”®ã€‘å‘ä½å¯¹é½äº†ï¼Œå†åŠ è½½ä½ çš„ Checkpoint
    if pretrained_path and os.path.exists(pretrained_path):
        print(f"  -> åŠ è½½æŒ‡å®š Checkpoint: {pretrained_path}")
        # ä¿®å¤ä¹‹å‰çš„ pickle æŠ¥é”™ï¼Œå…è®¸åŠ è½½éæƒé‡æ•°æ®
        checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)

        # æå– state_dict
        state_dict = checkpoint['model'] if isinstance(checkpoint, dict) and 'model' in checkpoint else checkpoint

        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(state_dict, strict=True)
            print("  âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
        except RuntimeError as e:
            # å¦‚æœ strict=True å¤±è´¥ï¼Œå°è¯• strict=False å¹¶æ‰“å°å¿½ç•¥çš„é”®
            print(f"  âš ï¸ ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éä¸¥æ ¼åŠ è½½: {e}")
            model.load_state_dict(state_dict, strict=False)
    else:
        if not pretrained_path:
            print("  -> ä½¿ç”¨ ImageNet é¢„è®­ç»ƒæƒé‡ (è¿ç§»å­¦ä¹ æ¨¡å¼)")

    return model

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    args = Args()
    print(f"ğŸ”§ CIFAR-10 å…¨å±€å¾®è°ƒ (Arch Modified) | Device: {DEVICE}")

    # --- å‡†å¤‡æµ‹è¯•é›† ---
    CIFAR_ROOT ='/root/autodl-tmp/data'
    print(f"ğŸ“š åŠ è½½æµ‹è¯•é›† (Root: {CIFAR_ROOT})...")
    try:
        test_dataset = torchvision.datasets.CIFAR10(
            root=CIFAR_ROOT, train=False, download=True, transform=get_cifar_transforms(is_train=False)
        )
    except:
        print("  âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=get_cifar_transforms(is_train=False)
        )

    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    # --- æœç´¢è®­ç»ƒæ–‡ä»¶ ---
    train_files = glob.glob(TRAIN_FILES_PATTERN)
    train_files.sort(key=lambda f: float(re.search(r'(\d+\.\d+)', f).group(1)) if re.search(r'(\d+\.\d+)', f) else 0.0)

    results = []

    for train_file in train_files:
        filename = os.path.basename(train_file)
        match = re.search(r'_(\d+\.\d+)\.pt', filename)
        ratio = match.group(1) if match else "Original"

        print("\n" + "=" * 60)
        print(f"ğŸš€ [ä»»åŠ¡] æ–‡ä»¶: {filename} | Ratio: {ratio}")
        print("=" * 60)

        # 1. å‡†å¤‡æ•°æ® (ä½¿ç”¨ CIFAR Transforms)
        dataset_train = FilelistDataset(train_file, transform=get_cifar_transforms(is_train=True))
        if len(dataset_train) == 0: continue
        loader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)

        # 2. å‡†å¤‡æ¨¡å‹ (åº”ç”¨ç»“æ„ä¿®æ”¹)
        model = get_cifar_resnet18(PRETRAINED_CHECKPOINT)
        model.to(DEVICE)

        # 3. ä¼˜åŒ–å™¨ (Global Finetune)
        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,
                                    momentum=args.momentum, weight_decay=args.weight_decay)
        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 12], gamma=0.1)
        criterion = nn.CrossEntropyLoss()

        # ... (æ¨¡å‹åˆå§‹åŒ–å’ŒåŠ è½½æƒé‡ä¹‹å) ...

        # === æ–°å¢ï¼šèµ·è·‘çº¿æ£€æŸ¥ ===
        print("\nğŸ” [Sanity Check] è®­ç»ƒå‰å…ˆæµ‹ä¸€æ¬¡ï¼Œç¡®è®¤èµ·ç‚¹æ˜¯å¦ä¸º 88%...")
        initial_metrics = evaluate(model, criterion, test_loader, device=DEVICE)
        print(f"ğŸ èµ·è·‘çº¿æˆç»©: Acc@1 {initial_metrics:.2f}%\n")
        # ========================



        # 4. è®­ç»ƒ
        best_acc = 0.0
        for epoch in range(args.epochs):
            train_one_epoch(model, criterion, optimizer, loader_train, DEVICE, epoch, args)
            lr_scheduler.step()

            # è¯„ä¼°
            eval_metrics = evaluate(model, criterion, test_loader, device=DEVICE)
            # ç›´æ¥èµ‹å€¼ï¼Œå› ä¸º eval_metrics å·²ç»æ˜¯è®¡ç®—å¥½çš„å‡†ç¡®ç‡äº†
            acc1 = eval_metrics
            best_acc = max(best_acc, acc1)

        print(f"âœ¨ ç»“æœ: Ratio {ratio} -> Best Acc: {best_acc:.2f}%")
        results.append([filename, ratio, f"{best_acc:.2f}"])

    # --- ä¿å­˜ ---
    with open('finetune_cifar_arch_results.csv', 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['Filename', 'Ratio', 'Best Acc'])
        w.writerows(results)
    print("\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()
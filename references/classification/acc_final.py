import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import sys
import glob
import re
import csv

# --- å¯¼å…¥åŒçº§ç›®å½•ä¸‹çš„å®˜æ–¹æ¨¡å— ---
try:
    import utils
    from train import train_one_epoch, evaluate
except ImportError:
    print("âŒ é”™è¯¯ï¼šè¯·å°†æ­¤è„šæœ¬æ”¾åœ¨ 'references/classification/' ç›®å½•ä¸‹è¿è¡Œï¼")
    sys.exit(1)

# ==========================================
# 1. é…ç½®åŒºåŸŸ
# ==========================================
TRAIN_FILES_PATTERN = 'eval_set_B*.pt'  # è®­ç»ƒé›†ç´¢å¼•æ–‡ä»¶æ¨¡å¼
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ä½ çš„ Checkpoint è·¯å¾„ (å¦‚æœéœ€è¦ä»ä¹‹å‰çš„æƒé‡å¼€å§‹ï¼Œè¯·å¡«è¿™é‡Œ)
# å¦‚æœç•™ç©ºï¼Œå°†é»˜è®¤ä½¿ç”¨ ImageNet é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–
PRETRAINED_CHECKPOINT = '/root/autodl-tmp/eval/references/classification/checkpoint.pth'


class Args:
    def __init__(self):
        self.batch_size = 128
        self.epochs = 15  # é’ˆå¯¹ CIFAR-10 ç»“æ„è°ƒæ•´åï¼Œè®­ç»ƒå¯èƒ½éœ€è¦ç¨å¤šå‡ è½®
        self.lr = 0.000001  # SGD åˆå§‹å­¦ä¹ ç‡
        self.momentum = 0.9
        self.weight_decay = 5e-4  # CIFAR-10 å¸¸ç”¨ weight_decay
        self.print_freq = 50
        self.workers = 8
        self.output_dir = '.'

        # å®˜æ–¹ä»£ç å ä½å‚æ•°
        self.clip_grad_norm = None
        self.model_ema = False
        self.lr_warmup_epochs = 0
        self.lr_warmup_method = 'linear'
        self.lr_warmup_decay = 0.01
        self.label_smoothing = 0.0
        self.mixup_alpha = 0.0
        self.cutmix_alpha = 0.0
        self.distributed = False
        self.rank = 0


# ==========================================
# 2. CIFAR-10 ä¸“ç”¨ Transforms
# ==========================================
def get_cifar_transforms(is_train=True):
    # CIFAR-10 æ ‡å‡†å‡å€¼å’Œæ–¹å·®
    mean = (0.4914, 0.4822, 0.4465)
    std = (0.2023, 0.1994, 0.2010)

    if is_train:
        return transforms.Compose([
            transforms.RandomCrop(32, padding=4),  # ä¿æŒ 32x32
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
    else:
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])


# ==========================================
# 3. è‡ªå®šä¹‰æ•°æ®é›† (åŒå‰)
# ==========================================
class FilelistDataset(Dataset):
    def __init__(self, pt_path, transform=None):
        self.samples = []
        self.transform = transform

        try:
            data = torch.load(pt_path, map_location='cpu')
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return

        items = data if isinstance(data, list) else []
        if isinstance(data, dict) and 'features' in data:
            ids = data['ids']
            lbls = data['labels']
            for i in range(len(ids)):
                items.append({'id': ids[i], 'label': int(lbls[i])})

        for item in items:
            raw_id = item.get('id')
            label = int(item.get('label'))
            if not raw_id: continue

            # è·¯å¾„æ¸…æ´—ï¼šå»é™¤ _copy_ åç¼€ï¼ŒæŒ‡å‘åŸå§‹å›¾ç‰‡
            real_path = raw_id
            if '_copy_' in real_path:
                real_path = real_path.split('_copy_')[0]

            self.samples.append((real_path, label))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        try:
            img = Image.open(path).convert('RGB')
        except:
            img = Image.new('RGB', (32, 32))

        if self.transform:
            img = self.transform(img)
        return img, label


# ==========================================
# 4. æ¨¡å‹ä¿®æ”¹å‡½æ•°
# ==========================================
# ==========================================
# 4. æ¨¡å‹ä¿®æ”¹å‡½æ•° (ä¿®æ­£ç‰ˆ)
# ==========================================
def get_cifar_resnet18(pretrained_path=''):
    # 1. æ— è®ºå¦‚ä½•ï¼Œå…ˆåˆå§‹åŒ–ä¸€ä¸ªæ ‡å‡† ResNet
    # å¦‚æœæœ‰è‡ªå®šä¹‰æƒé‡ï¼Œè¿™é‡Œå¯ä»¥ç”¨ weights=None çº¯å‡€åˆå§‹åŒ–
    # å¦‚æœæ²¡æœ‰ï¼Œç”¨ 'DEFAULT' åŠ è½½ ImageNet æƒé‡ä½œä¸ºè¿ç§»å­¦ä¹ çš„åŸºç¡€
    init_weights = None if pretrained_path else 'DEFAULT'
    model = torchvision.models.resnet18(weights=init_weights)

    # 2. ã€å…³é”®ã€‘å…ˆè¿›è¡Œæ¶æ„ä¿®æ”¹ï¼ŒæŠŠå‘ä½è°ƒæ•´å¥½
    # ä¿®æ”¹ç¬¬ä¸€å±‚å·ç§¯ï¼šé€‚åº” 32x32 è¾“å…¥ (7x7 -> 3x3)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)

    # ç§»é™¤ç¬¬ä¸€å±‚æ± åŒ–
    model.maxpool = nn.Identity()

    # ä¿®æ”¹å…¨è¿æ¥å±‚ï¼šé€‚åº” 10 ç±»
    model.fc = nn.Linear(model.fc.in_features, 10)

    # 3. ã€å…³é”®ã€‘å‘ä½å¯¹é½äº†ï¼Œå†åŠ è½½ä½ çš„ Checkpoint
    if pretrained_path and os.path.exists(pretrained_path):
        print(f"  -> åŠ è½½æŒ‡å®š Checkpoint: {pretrained_path}")
        # ä¿®å¤ä¹‹å‰çš„ pickle æŠ¥é”™ï¼Œå…è®¸åŠ è½½éæƒé‡æ•°æ®
        checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)

        # æå– state_dict
        state_dict = checkpoint['model'] if isinstance(checkpoint, dict) and 'model' in checkpoint else checkpoint

        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(state_dict, strict=True)
            print("  âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
        except RuntimeError as e:
            # å¦‚æœ strict=True å¤±è´¥ï¼Œå°è¯• strict=False å¹¶æ‰“å°å¿½ç•¥çš„é”®
            print(f"  âš ï¸ ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éä¸¥æ ¼åŠ è½½: {e}")
            model.load_state_dict(state_dict, strict=False)
    else:
        if not pretrained_path:
            print("  -> ä½¿ç”¨ ImageNet é¢„è®­ç»ƒæƒé‡ (è¿ç§»å­¦ä¹ æ¨¡å¼)")

    return model

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    args = Args()
    print(f"ğŸ”§ CIFAR-10 å…¨å±€å¾®è°ƒ (Arch Modified) | Device: {DEVICE}")

    # --- å‡†å¤‡æµ‹è¯•é›† ---
    CIFAR_ROOT ='/root/autodl-tmp/data'
    print(f"ğŸ“š åŠ è½½æµ‹è¯•é›† (Root: {CIFAR_ROOT})...")
    try:
        test_dataset = torchvision.datasets.CIFAR10(
            root=CIFAR_ROOT, train=False, download=True, transform=get_cifar_transforms(is_train=False)
        )
    except:
        print("  âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=get_cifar_transforms(is_train=False)
        )

    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    # --- æœç´¢è®­ç»ƒæ–‡ä»¶ ---
    train_files = glob.glob(TRAIN_FILES_PATTERN)
    train_files.sort(key=lambda f: float(re.search(r'(\d+\.\d+)', f).group(1)) if re.search(r'(\d+\.\d+)', f) else 0.0)

    results = []

    for train_file in train_files:
        filename = os.path.basename(train_file)
        match = re.search(r'_(\d+\.\d+)\.pt', filename)
        ratio = match.group(1) if match else "Original"

        print("\n" + "=" * 60)
        print(f"ğŸš€ [ä»»åŠ¡] æ–‡ä»¶: {filename} | Ratio: {ratio}")
        print("=" * 60)

        # 1. å‡†å¤‡æ•°æ® (ä½¿ç”¨ CIFAR Transforms)
        dataset_train = FilelistDataset(train_file, transform=get_cifar_transforms(is_train=True))
        if len(dataset_train) == 0: continue
        loader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)

        # 2. å‡†å¤‡æ¨¡å‹ (åº”ç”¨ç»“æ„ä¿®æ”¹)
        model = get_cifar_resnet18(PRETRAINED_CHECKPOINT)
        model.to(DEVICE)

        # 3. ä¼˜åŒ–å™¨ (Global Finetune)
        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,
                                    momentum=args.momentum, weight_decay=args.weight_decay)
        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 12], gamma=0.1)
        criterion = nn.CrossEntropyLoss()

        # ... (æ¨¡å‹åˆå§‹åŒ–å’ŒåŠ è½½æƒé‡ä¹‹å) ...

        # === æ–°å¢ï¼šèµ·è·‘çº¿æ£€æŸ¥ ===
        print("\nğŸ” [Sanity Check] è®­ç»ƒå‰å…ˆæµ‹ä¸€æ¬¡ï¼Œç¡®è®¤èµ·ç‚¹æ˜¯å¦ä¸º 88%...")
        initial_metrics = evaluate(model, criterion, test_loader, device=DEVICE)
        print(f"ğŸ èµ·è·‘çº¿æˆç»©: Acc@1 {initial_metrics:.2f}%\n")
        # ========================



        # 4. è®­ç»ƒ
        best_acc = 0.0
        for epoch in range(args.epochs):
            train_one_epoch(model, criterion, optimizer, loader_train, DEVICE, epoch, args)
            lr_scheduler.step()

            # è¯„ä¼°
            eval_metrics = evaluate(model, criterion, test_loader, device=DEVICE)
            # ç›´æ¥èµ‹å€¼ï¼Œå› ä¸º eval_metrics å·²ç»æ˜¯è®¡ç®—å¥½çš„å‡†ç¡®ç‡äº†
            acc1 = eval_metrics
            best_acc = max(best_acc, acc1)

        print(f"âœ¨ ç»“æœ: Ratio {ratio} -> Best Acc: {best_acc:.2f}%")
        results.append([filename, ratio, f"{best_acc:.2f}"])

    # --- ä¿å­˜ ---
    with open('finetune_cifar_arch_results.csv', 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['Filename', 'Ratio', 'Best Acc'])
        w.writerows(results)
    print("\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()